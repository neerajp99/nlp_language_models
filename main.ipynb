{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import nltk\n",
    "from nltk import trigrams, bigrams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('WarAndPeace.txt', 'r')\n",
    "new = f.read()\n",
    "new = new.strip('\\n').lower().replace(',', '')\n",
    "new = new.split()\n",
    "resultwords  = [word for word in new]\n",
    "result = ' '.join(resultwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise and create counter for unique words count\n",
    "tokenize_words = nltk.word_tokenize(result)\n",
    "count_elements = Counter(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get unigrams\n",
    "def get_unigrams(count_elements, lstr):\n",
    "    # Sort the tokenised words in descending order \n",
    "    desc_words = {val: v for val, v in sorted(count_elements.items(), reverse=True, key=lambda item: item[1])}\n",
    "    output = \"\"\n",
    "    ctr = 0\n",
    "    # Calculating maximum likelihood estimate (MLE)\n",
    "    desc_len = len(desc_words)\n",
    "    for word in desc_words:\n",
    "        desc_words[word] = desc_words[word] / desc_len\n",
    "    for i in desc_words:\n",
    "        if i.isalnum():\n",
    "            ctr += 1\n",
    "            output += i\n",
    "            output += \" \"\n",
    "        if ctr > 20:\n",
    "            break\n",
    "    return lstr + \" \" + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return bigram generated sentences\n",
    "def get_bigrams(tokenize_words, lstr):\n",
    "    # Method to get Bigrams generated sentences\n",
    "    model_bi = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    # Count frequency of co-occurance  \n",
    "    for word1, word2 in bigrams(tokenize_words):\n",
    "        model_bi[(word1)][word2] += 1\n",
    "\n",
    "   # Calculating maximum likelihood estimate (MLE)\n",
    "    for word1 in model_bi:\n",
    "        vocab_size = float(sum( model_bi[word1].values()))\n",
    "        for word2 in  model_bi[word1]:\n",
    "             model_bi[word1][word2] /= vocab_size\n",
    "                \n",
    "    # Create statements\n",
    "    text = [random.choice(list(model_bi))]\n",
    "    done = False \n",
    "    ctr = 0 # Counter element to break the loop\n",
    "\n",
    "    while not done:\n",
    "        ctr += 1\n",
    "        temp_check = dict()\n",
    "        # Using the maximum threshold for the probability\n",
    "        for word in dict(model_bi[text[-1]]):\n",
    "            temp_check[word] = model_bi[text[-1]][word]\n",
    "        desc_sorted = {val: v for val, v in sorted(temp_check.items(), reverse=True, key=lambda item: item[1])}\n",
    "        final_value = list(desc_sorted)[random.randint(0, len(temp_check) - 1)]\n",
    "        text.append(final_value)\n",
    "        \n",
    "        # If no word is available, break\n",
    "        if text[-1:] == [None, None]:\n",
    "            done = True\n",
    "        # Make a threshold of 15 words in a sentence\n",
    "        if ctr > 25:\n",
    "          done = True\n",
    "    # Ouput the sentence generates using Trigram\n",
    "    return lstr + ' ' + ' '.join([txt for txt in text if txt])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get trigram sentences\n",
    "def get_trigrams(tokenize_words, lstr):\n",
    "    # Create a placeholder for model\n",
    "    model_tri = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    # Count frequency of co-occurance\n",
    "    for word1, word2, word3 in trigrams(tokenize_words, pad_right=True, pad_left=True):\n",
    "        model_tri[(word1, word2)][word3] += 1\n",
    "\n",
    "    # Calculating maximum likelihood estimate (MLE)\n",
    "    for prev_words in model_tri:\n",
    "        vocab_size = float(sum(model_tri[prev_words].values()))\n",
    "        for word3 in model_tri[prev_words]:\n",
    "            model_tri[prev_words][word3] /= vocab_size\n",
    "\n",
    "    # Create sentences using it \n",
    "    words = random.choice(list(model_tri))\n",
    "    text = [words[0], words[1]]\n",
    "    done = False\n",
    "    ctr = 0\n",
    "\n",
    "    while not done:\n",
    "        ctr += 1\n",
    "      # Use the maximum threshold for the probbaility\n",
    "        temp_check = {}\n",
    "        for word in dict(model_tri[text[-2], text[-1]]):\n",
    "            temp_check[word] = model_tri[text[-2], text[-1]][word]\n",
    "        desc_sorted = {val: v for val, v in sorted(temp_check.items(), reverse=True, key=lambda item: item[1])}\n",
    "        final_value = list(desc_sorted)[random.randint(0, len(temp_check) - 1)]\n",
    "        text.append(final_value)\n",
    "\n",
    "        # If no more words available, break\n",
    "        if text[-2:] == [None, None]:\n",
    "          done = True\n",
    "\n",
    "        # Make a threshold of 15 words in a sentence\n",
    "        if ctr > 25:\n",
    "          done = True\n",
    "    # Ouput the sentence generates using Trigram\n",
    "    return lstr + ' ' + ' '.join([txt for txt in text if txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get four grams sentences\n",
    "def get_four_grams(result, lstr):\n",
    "    # Function to generate 4-grams from sentences.\n",
    "    def extract_ngrams(data, num):\n",
    "        word1, word2, word3 = \"\", \"\", \"\"\n",
    "        n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "        model_four = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for grams in n_grams:\n",
    "            word1 = grams[0]\n",
    "            word2 = grams[1]\n",
    "            word3 = grams[2]\n",
    "            word4 = grams[3]\n",
    "            # Count the frequency of occurance\n",
    "            model_four[(word1, word2, word3)][word4] += 1\n",
    "        return model_four\n",
    "    # Method to get Bigrams generated sentences\n",
    "    model_four = extract_ngrams(result, 4)\n",
    "   # Calculating maximum likelihood estimate (MLE)\n",
    "    for prev_words in model_four:\n",
    "        vocab_size = float(sum(model_four[prev_words].values()))\n",
    "        for word4 in model_four[prev_words]:\n",
    "            model_four[prev_words][word4] /= vocab_size\n",
    "    \n",
    "    # Initialise the start\n",
    "    words = random.choice(list(model_four))\n",
    "    text = [words[0], words[1], words[2]]\n",
    "    done = False \n",
    "    ctr = 0 # Counter element to break the loop\n",
    "    \n",
    "    while not done:\n",
    "        ctr += 1\n",
    "        temp_check = dict()\n",
    "        # Using the maximum threshold for the probability\n",
    "        for word in dict(model_four[text[-3], text[-2], text[-1]]):\n",
    "            temp_check[word] = model_four[text[-3], text[-2], text[-1]][word]\n",
    "        desc_sorted = {val: v for val, v in sorted(temp_check.items(), reverse=True, key=lambda item: item[1])}\n",
    "        final_value = list(desc_sorted)[random.randint(0, len(temp_check) - 1)]\n",
    "        text.append(final_value)\n",
    "        \n",
    "        # If no word is available, break\n",
    "        if text[-3:] == [None, None]:\n",
    "            done = True\n",
    "        # Make a threshold of 15 words in a sentence\n",
    "        if ctr > 25:\n",
    "          done = True\n",
    "    # Ouput the sentence generates using Trigram\n",
    "    return lstr + ' ' + ' '.join([txt for txt in text if txt])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not two minutes lucca are the buonapartes . with the sad current of importance who like him ... . perhaps i do count ( or prince so eloquent . all\n"
     ]
    }
   ],
   "source": [
    "print(get_bigrams(tokenize_words, \"Not two minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not two minutes she suddenly paused smiling at her own impetuosity . `` baron funke has been decided about novos√≠ltsev 's dispatch ? you know i did all a father could\n"
     ]
    }
   ],
   "source": [
    "print(get_trigrams(tokenize_words, \"Not two minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not two minutes daughter is coming for me to take me there . '' `` heavens ! what a virulent attack ! '' replied the prince in a cold listless tone .\n"
     ]
    }
   ],
   "source": [
    "print(get_four_grams(result, \"Not two minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not two minutes the and to of a i you he that is in she her with have prince do it said had not \n"
     ]
    }
   ],
   "source": [
    "print(get_unigrams(count_elements, \"Not two minutes\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
